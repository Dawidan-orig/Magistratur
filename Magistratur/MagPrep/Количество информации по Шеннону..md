Количество информации по Шеннону - это мера неопределённости, то-есть энтропия, для динамической разрядности (определяет основание логарифма) 

Смысл формулы: каждый исход несёт количество информации, обратно пропорциональное вероятности pipi (чем реже событие — тем больше информации в его появлении). Энтропия — это средневзвешенная сумма информации по всем возможным событиям с учётом их вероятностей.

Например, если все исходы равновероятны, формула Шеннона совпадает с формулой Хартли и вычисляется как $I=log_2N$.

![[Формула Хартли.]]